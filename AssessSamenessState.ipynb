{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, MultiLineString\n",
    "import numpy as np\n",
    "from arcgis.geometry._types import Polyline as esriPolyline\n",
    "from arcgis.geometry._types import Polygon as esriPolygon\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.features import GeoAccessor\n",
    "from arcgis.features.analysis import join_features\n",
    "from arcgis import features\n",
    "from getpass import getpass\n",
    "import json\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "import swifter\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcEditCodes(hucid, df):\n",
    "    print(f\"Starting on {hucid}\")\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    # Grab ADWR and NHD data within the same HUC\n",
    "    adwr_df = adwr_huc12[adwr_huc12.HUC12 == hucid]\n",
    "    nhd_df = nhdFlowlines_huc12[nhdFlowlines_huc12.HUC12 == hucid]\n",
    "    if len(adwr_df) == 0:\n",
    "        print(f\"no matching adwr watersheds for {hucid}\")\n",
    "        return None\n",
    "    elif len(nhd_df) == 0:\n",
    "        print(f\"no matching nhd watersheds for {hucid}\")\n",
    "        return None\n",
    "    #print(len(nhd_df), len(adwr_df))\n",
    "    #df[[\"EditCode\",\"EditNote\"]] = df.apply(lambda r: findIdentical(r, nhd_df, adwr_df), axis=1)# if r.PCLength == 0 else pd.Series([3,note]), axis=1)\n",
    "    df[[\"localNHD_idx\",\"localADWR_idx\"]] = df.apply(lambda r: findIdentical(r, nhd_df, adwr_df), axis=1)# if r.PCLength == 0 else pd.Series([3,note]), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def findLocalByLength(geom, ds, distance=10):\n",
    "    \"\"\"return list of stream lengths in ds where within distance value (meters, default 10)\n",
    "    and also whose length is within +- 15% of target length. The stream with maximum length\n",
    "    is returned\"\"\"\n",
    "    \n",
    "    t_length = geom.length\n",
    "    geom_b = geom.buffer(distance)\n",
    "    ds_l = ds[ds.intersects(geom_b)]\n",
    "    if len(ds_l) == 0:\n",
    "        return None\n",
    "        #return False\n",
    "    ds_l_c = gpd.clip(ds_l, geom_b)\n",
    "    ds_l_c[\"Length\"] = round(ds_l_c.geometry.length)\n",
    "    \n",
    "    # This checks for only identical.\n",
    "    #ds_l_c = ds_l_c[ds_l_c[\"Length\"] == round(geom.length)]\n",
    "    \n",
    "    # +- tolerance of 15%\n",
    "    geom_lowlim = t_length * 0.85\n",
    "    geom_highlim = t_length * 1.15\n",
    "    ds_l_c = ds_l_c[(ds_l_c[\"Length\"] >= geom_lowlim) & (ds_l_c[\"Length\"] <= geom_highlim)]\n",
    "    ds_l_c[\"LengthDiff\"] = ds_l_c.Length.apply(lambda l: abs(l-t_length))\n",
    "    ds_l_c.sort_values(by=\"LengthDiff\", ascending=True, inplace=True)\n",
    "    #lengths = ds_l_c[\"Length\"].values.tolist()\n",
    "    featIdxs = ds_l_c[\"Huc12FeatIdx\"].values.tolist()\n",
    "    \n",
    "    return \",\".join([str(fidx) for fidx in featIdxs])#lengths[:3]\n",
    "\n",
    "    \"\"\"\n",
    "    if len(lengths) == 1:\n",
    "        # only one match, so return the value\n",
    "        return ds_l_c[\"Length\"].values[0]\n",
    "        #return True\n",
    "    else:\n",
    "        return 0\n",
    "        #return False\n",
    "    \"\"\"\n",
    "\n",
    "def findIdentical(row, nhdDF, adwrDF):\n",
    "    idx = row.name\n",
    "    if idx % 10000 == 0 and idx != 0:\n",
    "        print(f\"On {idx} - {datetime.now()}\")\n",
    "    geom = row.geometry\n",
    "    nhd_local_idx = findLocalByLength(geom, nhdDF)\n",
    "    adwr_local_idx = findLocalByLength(geom, adwrDF)\n",
    "    \n",
    "    return pd.Series([nhd_local_idx, adwr_local_idx])\n",
    "    #if nhd_local and adwr_local:\n",
    "    #    return pd.Series([0, note]) \n",
    "    #else:\n",
    "    #    return pd.Series([3, note]) \n",
    "    \n",
    "    \n",
    "def calcSimilarity(row):\n",
    "    adeqLength = row.ADEQLength\n",
    "        \n",
    "    lengths = [row.NHDLength, row.NHDLength, row.ADWRLength, row.PCLength]\n",
    "    lengths.remove(0) if 0 in lengths else lengths\n",
    "    simScores = []\n",
    "    totalAllLengths = np.sum(lengths)\n",
    "    #print(\"lengths\", lengths)\n",
    "    for length in lengths:\n",
    "        absDiff = abs(adeqLength - length)\n",
    "        #print(\"AbsDiff\", absDiff)\n",
    "        simScore = absDiff/adeqLength\n",
    "        #print(\"simScore\", simScore)\n",
    "        lengthWeight = length/totalAllLengths\n",
    "        simScoreWeight = simScore * lengthWeight\n",
    "        simScores.append(simScoreWeight)\n",
    "    \n",
    "    simScoreF = 1-np.mean(simScores)\n",
    "    \n",
    "    return simScoreF\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def toGeodataFrame(sdf):\n",
    "    geomtype = [key for key in sdf.SHAPE.values[0].keys()][0]\n",
    "    if geomtype == \"rings\":\n",
    "        gType = esriPolygon\n",
    "    elif geomtype == \"paths\":\n",
    "        gType = esriPolyline\n",
    "    sdf[\"geometry\"] = sdf.SHAPE.apply(lambda g: MultiLineString(g[geomtype]) if type(g) is gType else np.NaN)\n",
    "    sdf = sdf[sdf[\"geometry\"].notnull()]\n",
    "    crs = sdf[\"SHAPE\"].values[0][\"spatialReference\"][\"latestWkid\"]\n",
    "    sdf_gdf = gpd.GeoDataFrame(sdf, geometry=\"geometry\", crs=f\"epsg:{crs}\")\n",
    "    return sdf_gdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def joinDFtoHUC12(flowline_df, huc12):\n",
    "    \"\"\"Overlaying all is very slow with GEOS, so spatial join\n",
    "    first, then find multiple joins (e.g. 2 to 1) and only overlay those\"\"\"\n",
    "    huc12 = huc12[[\"HUC12\", \"geometry\"]]\n",
    "    \n",
    "    flowline_df.reset_index(inplace=True)\n",
    "    flowline_df.rename(columns={\"index\":\"FeatIdx\"}, inplace=True)\n",
    "    \n",
    "    flowline_df_sjoin = gpd.sjoin(flowline_df, huc12, lsuffix=\"\")\n",
    "    \n",
    "    incolumns = flowline_df.columns.tolist()\n",
    "\n",
    "    flowlinedf_multi_join = flowline_df_sjoin[flowline_df_sjoin.FeatIdx.duplicated(keep=False)].copy()\n",
    "    flowlinedf_single_join = flowline_df_sjoin[~flowline_df_sjoin.FeatIdx.duplicated(keep=False)].copy()\n",
    "\n",
    "    # only keep columns that were in original before sjoin\n",
    "    flowlinedf_multi_join = flowlinedf_multi_join[incolumns]\n",
    "    # drop the duplicated feature resulting from the sjoin\n",
    "    flowlinedf_multi_join.drop_duplicates(subset=\"FeatIdx\", keep=\"first\", inplace=True)\n",
    "    # overlay on huc12s to split features\n",
    "    flowlinedf_multi_join = gpd.overlay(flowlinedf_multi_join, huc12, how='intersection')\n",
    "    \n",
    "    #Create Huc12FeatIdx after overlay so one unique for each flowline in each huc\n",
    "    flowline_df.reset_index(inplace=True)\n",
    "    flowline_df.rename(columns={\"index\":\"Huc12FeatIdx\"}, inplace=True)\n",
    "\n",
    "    outcolumns = incolumns + [\"HUC12\"]\n",
    "\n",
    "    # merge 1-1 join features and split features back together\n",
    "    flowlines_df_overlay = pd.concat([flowlinedf_single_join[outcolumns], flowlinedf_multi_join[outcolumns]])\n",
    "    \n",
    "    return flowlines_df_overlay\n",
    "\n",
    "\n",
    "def getNHDData(nhdData_loc, huc4Num):\n",
    "    folder = f\"NHDPLUS_H_{huc4Num}_HU4_GDB\"\n",
    "    gdb = f\"NHDPLUS_H_{huc4Num}_HU4_GDB.gdb\"\n",
    "    gdb_loc = os.path.join(nhdData_loc, folder, gdb)\n",
    "    if not os.path.exists(gdb_loc):\n",
    "        raise ValueError(f\"Unable to find the gdb {gdb_loc}\")\n",
    "    df_watershed = gpd.read_file(gdb_loc, layer=\"WBDHU4\")\n",
    "    df_watershed = df_watershed[df_watershed.HUC4 == str(huc4Num)].to_crs(\"epsg:3857\")\n",
    "    df_flowlines = gpd.read_file(gdb_loc, layer=\"NHDFlowline\").to_crs(\"epsg:3857\")\n",
    "    # should we filter by ftype 460 (\"stream/river\")?\n",
    "    \n",
    "    return df_watershed, df_flowlines\n",
    "\n",
    "\n",
    "def getADWRLayer():\n",
    "    \n",
    "    azgeo = \"https://azgeo.maps.arcgis.com/home/index.html\"\n",
    "    user = \"bhickson_azgeo\"\n",
    "    if 'password' not in globals():\n",
    "        password = getpass(f\"Provide password for user {user}:\")\n",
    "\n",
    "    gis = GIS(azgeo, user, password)\n",
    "\n",
    "    adwrItem = gis.content.get(\"ba7ac533920f47b4bbbb165772bbc7aa\")\n",
    "    \n",
    "    dfs = []\n",
    "    for i, layer in enumerate(adwrItem.layers):\n",
    "        print(f\"Getting layer {i}\")\n",
    "        adwr_sdf = layer.query().sdf\n",
    "        print(f\"Got layer {i}\")\n",
    "        dfs.append(toGeodataFrame(adwr_sdf))\n",
    "    \n",
    "    adwrLayers = pd.concat(dfs)\n",
    "    adwrLayers = adwrLayers.explode()\n",
    "\n",
    "    adwrLayers[\"PERMANENT_\"] = adwrLayers.apply(lambda r: r.PERMANENT if pd.isnull(r.PERMANENT_) else r.PERMANENT_, axis=1)\n",
    "    del adwrLayers[\"PERMANENT\"], adwrLayers[\"SHAPE_LEN\"], adwrLayers[\"SHAPE_LENG\"], adwrLayers[\"SHAPE\"]\n",
    "    adwrLayers.rename({\"PERMANENT_\": \"PERMANENT\", \"Shape__Length\": \"Shape_Length\", \"FID\": \"FeatureID\"}, inplace=True)\n",
    "    adwrLayers.FDATE = adwrLayers.FDATE.astype(str)\n",
    "    \n",
    "    adwrLayers.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return adwrLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TNMID</th>\n",
       "      <th>MetaSourceID</th>\n",
       "      <th>SourceDataDesc</th>\n",
       "      <th>SourceOriginator</th>\n",
       "      <th>SourceFeatureID</th>\n",
       "      <th>LoadDate</th>\n",
       "      <th>GNIS_ID</th>\n",
       "      <th>AreaAcres</th>\n",
       "      <th>AreaSqKm</th>\n",
       "      <th>States</th>\n",
       "      <th>HUC4</th>\n",
       "      <th>Name</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{C7D39B24-68A5-482B-9EC1-B20FEB452850}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-06-12T11:34:35</td>\n",
       "      <td>None</td>\n",
       "      <td>9650675.10</td>\n",
       "      <td>39054.93</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1507</td>\n",
       "      <td>Lower Gila</td>\n",
       "      <td>13.944801</td>\n",
       "      <td>3.779091</td>\n",
       "      <td>MULTIPOLYGON (((-112.15810 34.71632, -112.1580...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{238D719A-43E5-473F-AB01-2EC0CDA6F168}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-06-04T20:07:18</td>\n",
       "      <td>None</td>\n",
       "      <td>8714022.72</td>\n",
       "      <td>35264.43</td>\n",
       "      <td>AZ,UT</td>\n",
       "      <td>1407</td>\n",
       "      <td>Upper Colorado-Dirty Devil</td>\n",
       "      <td>12.874770</td>\n",
       "      <td>3.605345</td>\n",
       "      <td>MULTIPOLYGON (((-111.47581 39.12450, -111.4754...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{6CD2178B-03B1-4D2D-9BC5-B1328A200221}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-06-04T20:34:59</td>\n",
       "      <td>None</td>\n",
       "      <td>8622108.89</td>\n",
       "      <td>34892.47</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1506</td>\n",
       "      <td>Salt</td>\n",
       "      <td>19.642615</td>\n",
       "      <td>3.417107</td>\n",
       "      <td>MULTIPOLYGON (((-113.09819 35.86434, -113.0979...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    TNMID MetaSourceID SourceDataDesc  \\\n",
       "0  {C7D39B24-68A5-482B-9EC1-B20FEB452850}         None           None   \n",
       "1  {238D719A-43E5-473F-AB01-2EC0CDA6F168}         None           None   \n",
       "2  {6CD2178B-03B1-4D2D-9BC5-B1328A200221}         None           None   \n",
       "\n",
       "  SourceOriginator SourceFeatureID             LoadDate GNIS_ID   AreaAcres  \\\n",
       "0             None            None  2020-06-12T11:34:35    None  9650675.10   \n",
       "1             None            None  2020-06-04T20:07:18    None  8714022.72   \n",
       "2             None            None  2020-06-04T20:34:59    None  8622108.89   \n",
       "\n",
       "   AreaSqKm States  HUC4                        Name  Shape_Length  \\\n",
       "0  39054.93     AZ  1507                  Lower Gila     13.944801   \n",
       "1  35264.43  AZ,UT  1407  Upper Colorado-Dirty Devil     12.874770   \n",
       "2  34892.47     AZ  1506                        Salt     19.642615   \n",
       "\n",
       "   Shape_Area                                           geometry  \n",
       "0    3.779091  MULTIPOLYGON (((-112.15810 34.71632, -112.1580...  \n",
       "1    3.605345  MULTIPOLYGON (((-111.47581 39.12450, -111.4754...  \n",
       "2    3.417107  MULTIPOLYGON (((-113.09819 35.86434, -113.0979...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhd_gdb = r\"S:\\BHickson\\Shared\\NHD\\NHD_H_Arizona_State_GDB.gdb\"\n",
    "if not os.path.exists(nhd_gdb):\n",
    "    nhd_gdb = r\"./Data/NHD_H_Arizona_State_GDB/NHD_H_Arizona_State_GDB.gdb\"\n",
    "\n",
    "huc4s_t = gpd.read_file(nhd_gdb, layer=\"WBDHU4\")\n",
    "huc4s_t.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ IN NHD, ADWR, Arizona State, and HUC12s reproject to 3857 and clip HUCs to arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "adwr_loc = \"./Data/ADWR_Arizona.gpkg\"\n",
    "\n",
    "arizona = gpd.read_file(\"./Data/Arizona.gpkg\")\n",
    "huc12s = gpd.read_file(nhd_gdb, layer=\"WBDHU12\")\n",
    "nhdFlowlines = gpd.read_file(nhd_gdb, layer=\"NHDFlowline\")\n",
    "\n",
    "arizona.to_crs(\"epsg:3857\", inplace=True)\n",
    "huc12s.to_crs(\"epsg:3857\", inplace=True)\n",
    "nhdFlowlines.to_crs(\"epsg:3857\", inplace=True)\n",
    "\n",
    "huc12s_az = gpd.overlay(huc12s, arizona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split NHD, ADEQ, AND ADWR to HUC12 Boundaries and Join to HUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in ADEQ Data with HUC12 Joined\n",
      "Reading in ADWR Data with HUC12 Joined\n",
      "Reading in NHD Flowline Data with HUC12 Joined\n"
     ]
    }
   ],
   "source": [
    "adeq_huc12_loc = \"./Data/ADEQ_Arizona_HUC12.gpkg\"\n",
    "adwr_huc12_loc = \"./Data/ADEQ_Arizona_HUC12.gpkg\"\n",
    "nhd_huc12_loc = \"./Data/NHDFlowlines_HUC12.gpkg\"\n",
    "\n",
    "if os.path.exists(adeq_huc12_loc):\n",
    "    print(\"Reading in ADEQ Data with HUC12 Joined\")\n",
    "    adeq_huc12 = gpd.read_file(\"./Data/ADEQ_Arizona_HUC12.gpkg\")\n",
    "else:\n",
    "    adeq_arizona = gpd.read_file(r\"./Data/ADEQ_HR_Streams/NHD_HR_WBID.shp\")\n",
    "    adeq_arizona.to_crs(\"epsg:3857\", inplace=True)\n",
    "    adeq_arizona = adeq_arizona.explode()\n",
    "    adeq_huc12 = joinDFtoHUC12(adeq_arizona, huc12s_az)\n",
    "    adeq_huc12.to_file(\"./Data/ADEQ_Arizona_HUC12.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    \n",
    "if os.path.exists(adwr_huc12_loc):\n",
    "    print(\"Reading in ADWR Data with HUC12 Joined\")\n",
    "    adwr_huc12 = gpd.read_file(\"./Data/ADWR_Arizona_HUC12.gpkg\")\n",
    "else:\n",
    "    if not os.path.exists(adwr_loc):\n",
    "        print(\"Pulling down ADWR Data\")\n",
    "        adwr_data = getADWRLayer(adwr_item)\n",
    "        adwr_data.to_file(adwr_loc, driver=\"GPKG\")\n",
    "    else:\n",
    "        print(\"Reading in ADWR Data\")\n",
    "        adwr_data = gpd.read_file(adwr_loc)\n",
    "\n",
    "    adwr_data.to_crs(\"epsg:3857\")\n",
    "    adwr_huc12 = joinDFtoHUC12(adwr_data, huc12s_az)\n",
    "    adwr_huc12.to_file(\"./Data/ADWR_Arizona_HUC12.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "if os.path.exists(nhd_huc12_loc):\n",
    "    print(f\"Reading in NHD Flowline Data with HUC12 Joined\")\n",
    "    nhdFlowlines_huc12 = gpd.read_file(nhd_huc12_loc)\n",
    "else:\n",
    "    print(\"Joining Arizona HUC12s to NHD Flowlines\")\n",
    "    nhdFlowlines_huc12 = joinDFtoHUC12(nhdFlowlines, huc12s_az)\n",
    "    nhdFlowlines_huc12.to_file(nhd_huc12_loc, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unique ID (Huc12FeatIDx) after splitting by HUC by reseting Index for all NHD, ADEQ, and ADWR datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Huc12FeatIdx after overlay so one unique for each flowline in each huc\n",
    "for df in [adeq_huc12, adwr_huc12, nhdFlowlines_huc12]:\n",
    "    if \"Huc12FeatIdx\" not in df:\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={\"index\":\"Huc12FeatIdx\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADWR doesn't have watershed data over north-east Arizona. Pre filter ADEQ and NHD datasets so that we only compare areas where all three exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adeq_huc12 = adeq_huc12[adeq_huc12.HUC12.isin(adwr_huc12.HUC12.unique().tolist())]\n",
    "nhdFlowlines_huc12 = nhdFlowlines_huc12[nhdFlowlines_huc12.HUC12.isin(adwr_huc12.HUC12.unique().tolist())]\n",
    "\n",
    "adwr_huc12 = adwr_huc12[adwr_huc12.HUC12.isin(adeq_huc12.HUC12.unique().tolist())]\n",
    "nhdFlowlines_huc12 = nhdFlowlines_huc12[nhdFlowlines_huc12.HUC12.isin(adeq_huc12.HUC12.unique().tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NHD AND ADWR have irrationally small (less than 10cm). Remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(659957, 19) (481963, 16)\n"
     ]
    }
   ],
   "source": [
    "# remove some irrationally small linework\n",
    "nhdFlowlines_huc12.Shape_Length = nhdFlowlines_huc12.geometry.length\n",
    "adwr_huc12.Shape_Length = adwr_huc12.geometry.length\n",
    "\n",
    "nhdFlowlines_huc12 = nhdFlowlines_huc12[nhdFlowlines_huc12.Shape_Length > 0.1]\n",
    "adwr_huc12 = adwr_huc12[adwr_huc12.Shape_Length > 0.1]\n",
    "\n",
    "print(nhdFlowlines_huc12.shape, adwr_huc12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[calcEditCodes(huc12id, group) for huc12id, group in adeq_huc4_group.groupby(\"HUC12\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare NHD and ADWR to ADEQ and generate edit codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1408 at 2021-10-13 11:11:06.766307\n",
      "2 total HUC12s in HUC4 1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   53.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   53.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1501 at 2021-10-13 11:12:00.747617\n",
      "444 total HUC12s in HUC4 1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "C:\\Users\\BenJames\\anaconda3\\envs\\arcgis\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 45.2min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed: 88.1min\n",
      "[Parallel(n_jobs=-1)]: Done 444 out of 444 | elapsed: 153.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1502 at 2021-10-13 13:45:23.327533\n",
      "636 total HUC12s in HUC4 1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 44.9min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed: 89.0min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 143.9min\n",
      "[Parallel(n_jobs=-1)]: Done 636 out of 636 | elapsed: 216.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1503 at 2021-10-13 17:22:48.130738\n",
      "370 total HUC12s in HUC4 1503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 44.9min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed: 87.8min\n",
      "[Parallel(n_jobs=-1)]: Done 370 out of 370 | elapsed: 125.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1504 at 2021-10-13 19:29:12.230078\n",
      "239 total HUC12s in HUC4 1504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 45.3min\n",
      "[Parallel(n_jobs=-1)]: Done 239 out of 239 | elapsed: 83.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1505 at 2021-10-13 20:52:26.556462\n",
      "449 total HUC12s in HUC4 1505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 43.9min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed: 87.4min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 142.2min\n",
      "[Parallel(n_jobs=-1)]: Done 449 out of 449 | elapsed: 152.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1506 at 2021-10-13 23:25:38.381469\n",
      "414 total HUC12s in HUC4 1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed: 85.7min\n",
      "[Parallel(n_jobs=-1)]: Done 414 out of 414 | elapsed: 138.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1507 at 2021-10-14 01:44:32.667469\n",
      "378 total HUC12s in HUC4 1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 44.0min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed: 86.6min\n",
      "[Parallel(n_jobs=-1)]: Done 378 out of 378 | elapsed: 127.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HUC4 1508 at 2021-10-14 03:52:51.483032\n",
      "5 total HUC12s in HUC4 1508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   51.2s remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16h 43min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tday = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "note = f\"EditCode as assessed by Ben's script on {tday}\"\n",
    "\n",
    "adeq_huc12[\"HUC4\"] = adeq_huc12[\"HUC12\"].apply(lambda huc12: huc12[:4])\n",
    "\n",
    "files = []\n",
    "os.makedirs(f\"./Data/AssessmentByHUC4\", exist_ok=True)\n",
    "for huc4, adeq_huc4_group in adeq_huc12.groupby(\"HUC4\"):\n",
    "    print(f\"Starting HUC4 {huc4} at {datetime.now()}\")\n",
    "    ofile = f\"./Data/AssessmentByHUC4/adeq_huc12_lengthsComp_HUC4-{huc4}.gpkg\"\n",
    "    if os.path.exists(ofile):\n",
    "        print(f\"{ofile} exists\")\n",
    "        continue\n",
    "    else:\n",
    "        grps = adeq_huc4_group.groupby(\"HUC12\")\n",
    "        print(f\"{len(grps)} total HUC12s in HUC4 {huc4}\")\n",
    "        huc4_calcs = Parallel(n_jobs=-1, verbose=5, max_nbytes=None)(delayed(calcEditCodes)(huc12id, group) for huc12id, group in adeq_huc4_group.groupby(\"HUC12\"))\n",
    "        adeq_edits_j_concat = pd.concat(huc4_calcs)\n",
    "        adeq_edits_j_concat.to_file(ofile, driver=\"GPKG\")\n",
    "    files.append(ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [gpd.read_file(file) for file in files]\n",
    "dfs = pd.concat(dfs)\n",
    "dfs.rename(columns={\"local_NHDLength\": \"localNHD_idx\", \"local_ADWRLength\": \"localADWR_idx\"},inplace=True)\n",
    "dfs.to_file(\"./Data/AssessmentByHUC4/AllHUCs_LengthsComp_Arizona.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BenJames\\anaconda3\\envs\\arcgis\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0508673cb9134f15a3a2edaf6bc0446c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=503575.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b226f358685c447691c0ba0436ffed32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=503575.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def parseValues(v):\n",
    "    if pd.isnull(v):\n",
    "        idxes = [np.nan,np.nan,np.nan]\n",
    "    elif \",\" not in v:\n",
    "        idxes = [v,np.nan,np.nan]\n",
    "    else:\n",
    "        vs = v.split(\",\")[:3]\n",
    "        if len(vs) == 3:\n",
    "            idxes = vs\n",
    "        elif len(vs) == 2:\n",
    "            idxes = [vs[0], vs[1], np.nan]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Problem assigning values for {v}\")\n",
    "            \n",
    "    return pd.Series(idxes)\n",
    "            \n",
    "dfs[[\"NHDidx_1\", \"NHDidx_2\", \"NHD_idx_3\"]] = dfs.localNHD_idx.swifter.apply(lambda v: parseValues(v))\n",
    "dfs[[\"ADWRidx_1\", \"ADWRidx_2\", \"ADWRidx_3\"]] = dfs.localADWR_idx.swifter.apply(lambda v: parseValues(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = dfs.iloc[:10000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def getNHDLength(fidx):\n",
    "    if pd.isnull(fidx) or fidx == None or fidx==\"\":\n",
    "        return None\n",
    "    try:\n",
    "        l = nhdFlowlines_huc12[nhdFlowlines_huc12.Huc12FeatIdx == int(fidx)].geometry.length.values\n",
    "        if len(l) > 1:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            return l[0]\n",
    "    except:\n",
    "        print(fidx, pd.isnull(fidx),  fidx == None, fidx==\"\")\n",
    "        return None\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.NHDidx_1 = pd.to_numeric(dfs.NHDidx_1)\n",
    "dfs.ADWRidx_1 = pd.to_numeric(dfs.ADWRidx_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimal_precision = 3\n",
    "dfs2 = pd.merge(dfs, nhdFlowlines_huc12[[\"Huc12FeatIdx\", \"geometry\"]], left_on=\"NHDidx_1\", right_on=\"Huc12FeatIdx\", how=\"left\", suffixes=(\"\",\"_NHD\"))\n",
    "dfs2[\"localNHD_length\"] = dfs2.geometry_NHD.apply(lambda g: round(g.length,decimal_precision) if g != None else np.nan)\n",
    "\n",
    "dfs2 = pd.merge(dfs2, adwr_huc12[[\"Huc12FeatIdx\", \"geometry\"]], left_on=\"ADWRidx_1\", right_on=\"Huc12FeatIdx\", how=\"left\", suffixes=(\"\",\"_ADWR\"))\n",
    "dfs2[\"localADWR_length\"] = dfs2.geometry_ADWR.apply(lambda g: round(g.length, decimal_precision) if g != None else np.nan)\n",
    "del dfs2[\"Huc12FeatIdx_NHD\"], dfs2[\"geometry_NHD\"], dfs2[\"geometry_ADWR\"], dfs2[\"Huc12FeatIdx_ADWR\"]\n",
    "\n",
    "all_same = dfs2[(dfs2.localNHD_length==dfs2.geometry.length.round(decimal_precision)) & (dfs2.localADWR_length==dfs2.geometry.length.round(decimal_precision))]\n",
    "num_same = len(all_same)\n",
    "print(f\"{num_same} total streams are identical across three datasets ({round(100*num_same/len(dfs),1)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.to_file(\"./Data/AssessmentByHUC4/AllHUCs_LengthsComp_Arizona.gpkg\", driver=\"GPKG\")\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_same = dfs2[(dfs2.localNHD_length!=dfs2.geometry.length.round(decimal_precision)) | (dfs2.localADWR_length!=dfs2.geometry.length.round(decimal_precision))]\n",
    "not_same.to_file(\"./Data/AssessmentByHUC4/AllHUCs_NotSame.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310979 streams of 503575 total streams are identical (61.75%)\n",
      "\t57.1% by stream length\n"
     ]
    }
   ],
   "source": [
    "threes = len(dfs[dfs.EditCode == 3])\n",
    "zeros = len(dfs[dfs.EditCode == 0])\n",
    "zeros_length = dfs[dfs.EditCode == 0].geometry.length.sum()\n",
    "total_length = dfs.geometry.length.sum()\n",
    "\n",
    "print(f\"{zeros} streams of {len(dfs)} total streams are identical ({round(zeros/len(dfs)*100,2)}%)\\n\\t{round(100*zeros_length/total_length,2)}% by stream length\")\n",
    "#print(f\"{threes} streams of {len(dfs)} total streams to be checked ({round(threes/len(dfs)*100,2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TNMID</th>\n",
       "      <th>MetaSourceID</th>\n",
       "      <th>SourceDataDesc</th>\n",
       "      <th>SourceOriginator</th>\n",
       "      <th>SourceFeatureID</th>\n",
       "      <th>LoadDate</th>\n",
       "      <th>GNIS_ID</th>\n",
       "      <th>AreaAcres</th>\n",
       "      <th>AreaSqKm</th>\n",
       "      <th>States</th>\n",
       "      <th>HUC8</th>\n",
       "      <th>Name</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{21A6AE9C-8C85-4578-AE5A-368093D0E9E5}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-06-11T07:54:56</td>\n",
       "      <td>None</td>\n",
       "      <td>1587400.21</td>\n",
       "      <td>6423.99</td>\n",
       "      <td>AZ,MX</td>\n",
       "      <td>15050202</td>\n",
       "      <td>Upper San Pedro</td>\n",
       "      <td>5.472283</td>\n",
       "      <td>0.610343</td>\n",
       "      <td>MULTIPOLYGON (((-12252803.888 3804351.247, -12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>{EE94F429-A5EE-4583-8558-00435EF322A7}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-06-04T20:32:35</td>\n",
       "      <td>None</td>\n",
       "      <td>1680515.46</td>\n",
       "      <td>6800.81</td>\n",
       "      <td>AZ,MX</td>\n",
       "      <td>15050301</td>\n",
       "      <td>Upper Santa Cruz</td>\n",
       "      <td>6.245822</td>\n",
       "      <td>0.647308</td>\n",
       "      <td>MULTIPOLYGON (((-12341933.794 3850142.150, -12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     TNMID MetaSourceID SourceDataDesc  \\\n",
       "40  {21A6AE9C-8C85-4578-AE5A-368093D0E9E5}         None           None   \n",
       "52  {EE94F429-A5EE-4583-8558-00435EF322A7}         None           None   \n",
       "\n",
       "   SourceOriginator SourceFeatureID             LoadDate GNIS_ID   AreaAcres  \\\n",
       "40             None            None  2012-06-11T07:54:56    None  1587400.21   \n",
       "52             None            None  2020-06-04T20:32:35    None  1680515.46   \n",
       "\n",
       "    AreaSqKm States      HUC8              Name  Shape_Length  Shape_Area  \\\n",
       "40   6423.99  AZ,MX  15050202   Upper San Pedro      5.472283    0.610343   \n",
       "52   6800.81  AZ,MX  15050301  Upper Santa Cruz      6.245822    0.647308   \n",
       "\n",
       "                                             geometry  \n",
       "40  MULTIPOLYGON (((-12252803.888 3804351.247, -12...  \n",
       "52  MULTIPOLYGON (((-12341933.794 3850142.150, -12...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huc8s = gpd.read_file(r\"S:\\BHickson\\Shared\\NHD\\NHD_H_Arizona_State_GDB.gdb\", layer=\"WBDHU8\")\n",
    "huc8s = huc8s[(huc8s[\"HUC8\"] == \"15050301\") | (huc8s[\"HUC8\"] == \"15050202\")].to_crs(huc_15050301.crs)\n",
    "huc8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc_15050301 = gpd.read_file(\"./Data/AssessmentByHUC4/adeq_huc12_HUC4-1505.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tdf = gpd.overlay(huc_15050301, huc8s)\n",
    "\n",
    "tdf.to_file(\"./Data/PilotStreams.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38206, 39)\n",
      "(24829, 39)\n"
     ]
    }
   ],
   "source": [
    "print(tdf.shape)\n",
    "print(tdf[tdf.EditCode == 0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
